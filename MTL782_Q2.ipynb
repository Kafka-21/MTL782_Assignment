{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Implement Apriori and FP-growth algorithm. Cite any sources helpful to you for implementing the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implmenting Apriori Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating transaction array \n",
    "transactions = []\n",
    "\n",
    "# reading data from retail.dat file line-wise and storing each line as transcation in transcation array\n",
    "with open('/Users/quasar/Downloads/Courses/Data Mining/retail.dat') as f:\n",
    "  for line in f.readlines():\n",
    "    transaction = line.strip().split(' ')\n",
    "    transaction = np.asarray(transaction, dtype='int64')\n",
    "    transactions.append(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([30, 31, 32]),\n",
       " array([33, 34, 35]),\n",
       " array([36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]),\n",
       " array([38, 39, 47, 48]),\n",
       " array([38, 39, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]),\n",
       " array([32, 41, 59, 60, 61, 62]),\n",
       " array([ 3, 39, 48]),\n",
       " array([63, 64, 65, 66, 67, 68]),\n",
       " array([32, 69])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing subset of transactions array to reduce compute time \n",
    "transactions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 1-item candidate, \n",
    "# Function creates a frozenset for each unique item and stores them in a list \n",
    "   \n",
    "def create_candidate_1(X):\n",
    "          c1 = []\n",
    "          for transaction in X:\n",
    "                    for t in transaction:\n",
    "                              t = frozenset([t])\n",
    "                              if t not in c1: \n",
    "                                        c1.append(t)\n",
    "          return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data is input to the function and the minimum support threshold to obtain the frequent itemset. \n",
    "# Functions store the support for each itemset, which will be used in the rule generation step\n",
    "          \n",
    "# the candidate sets for the 1-item is different\n",
    "# create them independently from others\n",
    "\n",
    "def apriori(X, min_support):          \n",
    "          c1 = create_candidate_1(X)\n",
    "          freq_item, item_support_dict = create_freq_item(X, c1, min_support)\n",
    "          freq_items = [freq_item]\n",
    "\n",
    "          k=0\n",
    "          while len(freq_items[k]) > 0:\n",
    "                    freq_item = freq_items[k]\n",
    "                    ck = create_candidate_k(freq_item, k)\n",
    "                    freq_item, item_support = create_freq_item(X, ck, min_support)\n",
    "                    freq_items.append(freq_item)\n",
    "                    item_support_dict.update(item_support)\n",
    "                    k += 1\n",
    "\n",
    "          return freq_items, item_support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters the candidate with the specified minimum support \n",
    "# loop through the transcation and compute the count for each candidate (item)\n",
    "# if the support of an item is greater than the min_support, then it is considered as frequent \n",
    "\n",
    "def create_freq_item(X, ck, min_support = 0.2):\n",
    "          item_count = {}\n",
    "          for transcation in X:\n",
    "                    for item in ck:\n",
    "                              if item.issubset(transcation):\n",
    "                                        if item not in item_count:\n",
    "                                                  item_count[item] = 1\n",
    "                                        else:\n",
    "                                                  item_count[item] += 1\n",
    "          \n",
    "          n_row = len(X)\n",
    "          freq_item = []\n",
    "          item_support = {}\n",
    "\n",
    "          for item in item_count:\n",
    "                    support = item_count[item] / n_row\n",
    "                    if support >= min_support:\n",
    "                              freq_item.append(item)\n",
    "                    \n",
    "                    item_support[item] = support \n",
    "          \n",
    "          return freq_item, item_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of k-item candidate\n",
    "          \n",
    "def create_candidate_k(freq_item, k):          \n",
    "          ck = []\n",
    "          # for generating candidate of size two (2-itemset)\n",
    "          if k == 0:\n",
    "                    for f1, f2 in combinations(freq_item, 2):\n",
    "                              item = f1 | f2 # union of two sets\n",
    "                              ck.append(item)\n",
    "          else:\n",
    "                    for f1, f2 in combinations(freq_item, 2):\n",
    "                              # if the two (k+1)-item sets has k common elements then they will be unioned to be \n",
    "                              # the (k+2)- item candidate\n",
    "                              intersection = f1 & f2\n",
    "                              if len(intersection) == k:\n",
    "                                        item = f1 | f2\n",
    "                                        if item not in ck:\n",
    "                                                  ck.append(item)\n",
    "\n",
    "          return ck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({38}), frozenset({39}), frozenset({41}), frozenset({48})],\n",
       " [frozenset({39, 48})],\n",
       " []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_items, item_support_dict =  apriori(transactions[:500], min_support = 0.2)\n",
    "freq_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the association rules, the rules will be a list.\n",
    "# each element is a tuple of size 4, containing rules' left hand side, right hand side, confidence \n",
    "\n",
    "def create_rules(freq_items, item_support_dict, min_confidence):\n",
    "    association_rules = []\n",
    "\n",
    "    # for the list that stores the frequent items, loop through\n",
    "    # the second element to the one before the last to generate the rules\n",
    "    # because the last one will be an empty list. It's the stopping criteria\n",
    "    # for the frequent itemset generating process and the first one are all\n",
    "    # single element frequent itemset, which can't perform the set\n",
    "    # operation X -> Y - X\n",
    "    \n",
    "    for idx, freq_item in enumerate(freq_items[1:(len(freq_items) - 1)]):\n",
    "        for freq_set in freq_item:\n",
    "            \n",
    "            # start with creating rules for single item on\n",
    "            # the right hand side\n",
    "            subsets = [frozenset([item]) for item in freq_set]\n",
    "            rules, right_hand_side = compute_conf(freq_items, item_support_dict, \n",
    "                                                  freq_set, subsets, min_confidence)\n",
    "            association_rules.extend(rules)\n",
    "            \n",
    "            # starting from 3-itemset, loop through each length item\n",
    "            # to create the rules, as for the while loop condition,\n",
    "            # e.g. suppose you start with a 3-itemset {2, 3, 5} then the \n",
    "            # while loop condition will stop when the right hand side's\n",
    "            # item is of length 2, e.g. [ {2, 3}, {3, 5} ], since this\n",
    "            # will be merged into 3 itemset, making the left hand side\n",
    "            # null when computing the confidence\n",
    "            if idx != 0:\n",
    "                k = 0\n",
    "                while len(right_hand_side[0]) < len(freq_set) - 1:\n",
    "                    ck = create_candidate_k(right_hand_side, k = k)\n",
    "                    rules, right_hand_side = compute_conf(freq_items, item_support_dict,\n",
    "                                                          freq_set, ck, min_confidence)\n",
    "                    association_rules.extend(rules)\n",
    "                    k += 1    \n",
    "    \n",
    "    return association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the rules and returns the rules info and the rules's right hand side \n",
    "# (used for generating the next round of rules) \n",
    "# if it surpasses the minimum confidence threshold\n",
    "\n",
    "def compute_conf(freq_items, item_support_dict, freq_set, subsets, min_confidence):\n",
    "    rules = []\n",
    "    right_hand_side = []\n",
    "    \n",
    "    for rhs in subsets:\n",
    "        # create the left hand side of the rule\n",
    "        # and add the rules if it's greater than\n",
    "        # the confidence threshold\n",
    "        lhs = freq_set - rhs\n",
    "        conf = item_support_dict[freq_set] / item_support_dict[lhs]\n",
    "        if conf >= min_confidence:\n",
    "            lift = conf / item_support_dict[rhs]\n",
    "            rules_info = lhs, rhs, conf, lift\n",
    "            rules.append(rules_info)\n",
    "            right_hand_side.append(rhs)\n",
    "            \n",
    "    return rules, right_hand_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({39}), frozenset({48}), 0.574750830564784, 1.2549144772156857),\n",
       " (frozenset({48}), frozenset({39}), 0.7554585152838427, 1.2549144772156855)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rules = create_rules(freq_items, item_support_dict, min_confidence = 0.5)\n",
    "association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing FP aglorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of FP tree node \n",
    "class Tree_Node:\n",
    "          def __init__(self, node_name, counter, parent_node):\n",
    "                    self.name = node_name\n",
    "                    self.count = counter \n",
    "                    self.node_link = None \n",
    "                    self.parent = parent_node\n",
    "                    self.children = {}\n",
    "          \n",
    "          def increment_counter(self, counter):\n",
    "                    self.count += counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_FP_tree(dataset, min_support):\n",
    "          header_table = {}\n",
    "          for transaction in dataset:\n",
    "                    for item in transaction:\n",
    "                              header_table[item] = header_table.get(item, 0) + dataset[transaction]\n",
    "          for k in list(header_table):\n",
    "                    if header_table[k] / len(dataset) < min_support:\n",
    "                              del(header_table[k])\n",
    "\n",
    "          frequent_itemset = set(header_table.keys())\n",
    "\n",
    "          if len(frequent_itemset) == 0:\n",
    "                    return None, None\n",
    "\n",
    "          for k in header_table:\n",
    "                    header_table[k] = [header_table[k], None]\n",
    "\n",
    "          retTree = Tree_Node('Null Set', 1, None)\n",
    "\n",
    "          for itemset, count in dataset.items():\n",
    "                    frequent_transaction = {}\n",
    "                    #print(dataset.items())\n",
    "                    #print(itemset, count)\n",
    "                    for item in itemset:\n",
    "                              if item in frequent_itemset:\n",
    "                                        frequent_transaction[item] = header_table[item][0]\n",
    "                                        #print(frequent_transaction)\n",
    "\n",
    "                                        \n",
    "\n",
    "create_FP_tree(initSet, min_support=0.1)\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create header_table and ordered itemsets for FP Tree\n",
    "def create_FP_tree(dataset, min_support):\n",
    "          header_table = {}\n",
    "          for transaction in dataset:\n",
    "                    for item in transaction:\n",
    "                              header_table[item] = header_table.get(item, 0) + dataset[transaction]\n",
    "          for k in list(header_table):\n",
    "                    if header_table[k] / len(dataset) < min_support:\n",
    "                              del(header_table[k])\n",
    "\n",
    "          frequent_itemset = set(header_table.keys())\n",
    "\n",
    "          if len(frequent_itemset) == 0:\n",
    "                    return None, None\n",
    "\n",
    "          for k in header_table:\n",
    "                    header_table[k] = [header_table[k], None]\n",
    "\n",
    "          retTree = Tree_Node('Null Set', 1, None)\n",
    "\n",
    "          for itemset, count in dataset.items():\n",
    "                    frequent_transaction = {}\n",
    "                    for item in itemset:\n",
    "                              if item in frequent_itemset:\n",
    "                                        frequent_transaction[item] = header_table[item][0]\n",
    "                    if len(frequent_transaction) > 0:\n",
    "                              # to get ordered itemset from transactions \n",
    "                              ordered_itemset = [v[0] for v in sorted(frequent_transaction.items(), key=lambda p: p[1], reverse=True)]\n",
    "                              # to update the FP Tree\n",
    "                              update_tree(ordered_itemset, retTree, header_table, count)\n",
    "          return retTree, header_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update the FP Tree using ordered itemsets\n",
    "def update_tree(itemset, FP_tree, header_table, count):\n",
    "          if itemset[0] in FP_tree.children:\n",
    "                    FP_tree.children[itemset[0]].increment_counter(count)\n",
    "          else: \n",
    "                    FP_tree.children[itemset[0]] = Tree_Node(itemset[0], count, FP_tree)\n",
    "\n",
    "                    if header_table[itemset[0]][1] == None:\n",
    "                              header_table[itemset[0]][1] = FP_tree.children[itemset[0]]\n",
    "                    else:\n",
    "                              update_Node_link(header_table[itemset[0]][1], FP_tree.children[itemset[0]])\n",
    "          \n",
    "          if len(itemset) > 1:\n",
    "                    update_tree(itemset[1::], FP_tree.children[itemset[0]], header_table, count)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To update the link of node in FP tree\n",
    "def update_Node_link(test_node, target_node):\n",
    "          while(test_node.node_link != None):\n",
    "                    test_node = test_node.node_link\n",
    "\n",
    "          test_node.node_link = target_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to transverse FP_tree in upward direction \n",
    "def FP_tree_uptransversal(leaf_node, prefix_Path):\n",
    "          if leaf_node.parent != None:\n",
    "                    prefix_Path.append(leaf_node.name)\n",
    "                    FP_tree_uptransversal(leaf_node.parent, prefix_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find conditional Pattern base\n",
    "def find_prefix_path(base_Pattern, Tree_Node):\n",
    "          conditional_patterns_base = {}\n",
    "\n",
    "          while Tree_Node != None:\n",
    "                    prefix_Path = []\n",
    "                    FP_tree_uptransversal(Tree_Node, prefix_Path)\n",
    "                    if len(prefix_Path) > 1:\n",
    "                              conditional_patterns_base[frozenset(prefix_Path[1:])] = Tree_Node.count \n",
    "                    Tree_Node = Tree_Node.node_link\n",
    "          \n",
    "          return conditional_patterns_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to mine recursively conditional patterns base and conditional FP tree\n",
    "def Mine_tree(FPTree, header_table, min_support, prefix, frequent_itemset):\n",
    "    bigL = [v[0] for v in sorted(header_table.items(),key=lambda p: p[1][0])]\n",
    "    for base_Pattern in bigL:\n",
    "        new_frequentset = prefix.copy()\n",
    "        new_frequentset.add(base_Pattern)\n",
    "        #add frequent itemset to final list of frequent itemsets\n",
    "        frequent_itemset.append(new_frequentset)\n",
    "        #get all conditional pattern bases for item or itemsets\n",
    "        Conditional_pattern_bases = find_prefix_path(base_Pattern, header_table[base_Pattern][1])\n",
    "        #call FP Tree construction to make conditional FP Tree\n",
    "        Conditional_FP_tree, Conditional_header = create_FP_tree(Conditional_pattern_bases,min_support)\n",
    "\n",
    "        if Conditional_header != None:\n",
    "            Mine_tree(Conditional_FP_tree, Conditional_header, min_support, new_frequentset, frequent_itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load file and return lists of Transactions\n",
    "def Load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    content = [x.strip() for x in content]\n",
    "    Transaction = []\n",
    "\n",
    "    for i in range(0, len(content)):\n",
    "        Transaction.append(content[i].split())\n",
    "\n",
    "    return Transaction\n",
    "    \n",
    "#To convert initial transaction into frozenset\n",
    "def create_initialset(dataset):\n",
    "    retDict = {}\n",
    "    for trans in dataset:\n",
    "        retDict[frozenset(trans)] = 1\n",
    "    return retDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Tree_Node object at 0x108934fd0> {'3': [2, <__main__.Tree_Node object at 0x122676700>], '32': [3, <__main__.Tree_Node object at 0x10b6e0d90>], '41': [2, <__main__.Tree_Node object at 0x124190d30>], '38': [3, <__main__.Tree_Node object at 0x1241902b0>], '39': [4, <__main__.Tree_Node object at 0x1241902e0>], '48': [3, <__main__.Tree_Node object at 0x124190760>]}\n"
     ]
    }
   ],
   "source": [
    "min_support = 0.2\n",
    "\n",
    "filename = '/Users/quasar/Downloads/Courses/Data Mining/retail.dat'\n",
    "initSet = create_initialset(Load_data(filename)[:10])\n",
    "FP_tree, header_table = create_FP_tree(initSet, min_support)\n",
    "print(FP_tree, header_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'3'}, {'39', '3'}, {'48', '3'}, {'39', '48', '3'}, {'41'}, {'38', '41'}, {'39', '41'}, {'38', '39', '41'}, {'32', '41'}, {'32'}, {'38'}, {'38', '39'}, {'48'}, {'38', '48'}, {'38', '39', '48'}, {'39', '48'}, {'39'}]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemset = []\n",
    "\n",
    "#call function to mine all frequent itemsets\n",
    "Mine_tree(FP_tree, header_table, min_support, set([]), frequent_itemset)\n",
    "print(frequent_itemset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Modify the algorithms to achieve the same task (preferably with some improvement) . Clearly mention the difference in the modified algorithm.\n",
    "\n",
    "Implementation Apriori using Hashtree"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
